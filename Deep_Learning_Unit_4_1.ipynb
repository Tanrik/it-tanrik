{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 수치 미분"
      ],
      "metadata": {
        "id": "MwhYPolXzcaV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "진정한 미분보다 수치 미분은 오차가 포함되기 때문에 이 오차를 줄이기 위해 (x+h)와(x-h)일 때 함수 f의 차분을 계산하는 방법으로 쓴다 이를 구현하면"
      ],
      "metadata": {
        "id": "-jMJoYwfzz2-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rr33mP1kwI5y"
      },
      "outputs": [],
      "source": [
        "def numerical_diff(f, x):\n",
        "  h = 1e-4\n",
        "  return (f(x+h) - f(x-h)) / (2*h)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "수치 미분의 예"
      ],
      "metadata": {
        "id": "MEAaZ2BnzbXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def numerical_diff(f, x):\n",
        "  h = 1e-4\n",
        "  return (f(x+h) - f(x-h)) / (2*h)\n",
        "\n",
        "def function_1(x):\n",
        "  return 0.01*x**2 + 0.1*x       \n",
        "\n",
        "numerical_diff(function_1,5)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNL2r19G6TZA",
        "outputId": "578d3c8e-8a27-4143-90e9-8d9629ff4001"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1999999999990898"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 편미분"
      ],
      "metadata": {
        "id": "IF82PlpSAoez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "변수가 여럿인 함수에 대한 미분"
      ],
      "metadata": {
        "id": "zUhoK6RtAsYR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "예시"
      ],
      "metadata": {
        "id": "XE7F0HzkBIUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# x0 = 3 x1 = 4 일때 (x0 기준)\n",
        "def numerical_diff(f, x):\n",
        "  h = 1e-4\n",
        "  return (f(x+h) - f(x-h)) / (2*h)\n",
        "\n",
        "def function_tmp1(x0):\n",
        "  return x0*x0 + 4.0**2.0\n",
        "\n",
        "print('x0', numerical_diff(function_tmp1, 3.0))\n",
        "\n",
        "# x1 기준\n",
        "\n",
        "def function_tmp2(x1):\n",
        "  return 3.0**2.0 + x1*x1\n",
        "\n",
        "print('x1', numerical_diff(function_tmp2, 4.0)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ktxqr_PdArr9",
        "outputId": "43190102-612b-4b40-e455-ab97b827053d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x0 6.00000000000378\n",
            "x1 7.999999999999119\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "편미분은 변수가 하나인 미분과 마찬가지로 기울기를 구한다. 단, 여러 변수 중 목표 변수 하나에 초점을 맞추고 다른 변수는 값을 고정한다. 그렇기 때문에 앞의 예에서는 목표 변수를 제외한 나머지를 특정 값에 고정하기 위해서 새로운 함수를 정의 한 것이다. "
      ],
      "metadata": {
        "id": "gdlqVs3TDZbZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 기울기"
      ],
      "metadata": {
        "id": "rTfPJH8Z9yya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "모든 변수(위 예시로는 x0, x1 의 모든 변수)의 편미분을 벡터로 정리한 것 "
      ],
      "metadata": {
        "id": "eiB_aBIN-MRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "예시"
      ],
      "metadata": {
        "id": "Om6H5akH-P3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.lib.function_base import gradient\n",
        "import numpy as np\n",
        "\n",
        "def numerical_gradient(f, x):\n",
        "    h = 1e-4\n",
        "    grad = np.zeros_like(x)\n",
        "\n",
        "    for idx in range(x.size):\n",
        "        tmp_val = x[idx]\n",
        "\n",
        "        x[idx] = tmp_val + h\n",
        "        fxh1 = f(x) \n",
        "\n",
        "        x[idx] = tmp_val - h\n",
        "        fxh2 = f(x)\n",
        "\n",
        "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
        "\n",
        "    return grad\n",
        "\n",
        "def function_2(x):\n",
        "    return x[0]**2 + x[1]**2\n",
        "\n",
        "numerical_gradient(function_2, np.array([3.0, 4.0]))\n",
        "\n"
      ],
      "metadata": {
        "id": "EFSHA_o1FDaB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d491f8c4-7fdd-48d1-da5e-4dd58a0cb5eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([6., 8.])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이러한 기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향이다."
      ],
      "metadata": {
        "id": "SIBkIJcxCt1Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 경사법"
      ],
      "metadata": {
        "id": "t3r9NyLZiYpR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "꼭 기울어진 방향이 최솟값을 가리키는 것은 아니나, 그 방향으로 가야 함수의 값을 줄일 수 있습니다. 그렇기 때문에 일정 거리는 가면서 기울기를 측정해 최솟값에 다가가야한다. 이러한 방식을 경사법이라고 불린다. 또한 최솟값은 경사 하강법 최댓값은 경사 상승법이라고한다."
      ],
      "metadata": {
        "id": "2Gpe2NvDicu4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "예시"
      ],
      "metadata": {
        "id": "yINU8zDPmTfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def numerical_gradient(f, x):\n",
        "    h = 1e-4\n",
        "    grad = np.zeros_like(x)\n",
        "\n",
        "    for idx in range(x.size):\n",
        "        tmp_val = x[idx]\n",
        "\n",
        "        x[idx] = tmp_val + h\n",
        "        fxh1 = f(x) \n",
        "\n",
        "        x[idx] = tmp_val - h\n",
        "        fxh2 = f(x)\n",
        "\n",
        "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
        "\n",
        "    return grad\n",
        "\n",
        "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
        "    x = init_x\n",
        "\n",
        "    for i in range(step_num):\n",
        "        grad = numerical_gradient(f, x)\n",
        "        x -= lr * grad\n",
        "    return x\n",
        "\n",
        "def function_2(x):\n",
        "    return x[0]**2 + x[1]**2\n",
        "\n",
        "\n",
        "init_x = np.array([-3.0, 4.0])\n",
        "gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)"
      ],
      "metadata": {
        "id": "3dGW3ZpXGqz0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa5d20d4-24b8-43de-e223-096d5ddac94e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.0005, -0.0005])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "lr은 학습률을 나타낸다. 다만 이 값이 너무 크거나 너무 작은 경우 발산해 버리거나 값이 갱신되지 않고 끝나 버린다."
      ],
      "metadata": {
        "id": "nj2d3U8Kzi_H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 신경망에서의 기울기"
      ],
      "metadata": {
        "id": "G05VpByX1xx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, os\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks/deep-learning-from-scratch-master') \n",
        "import numpy as np\n",
        "from common.functions import softmax, cross_entropy_error\n",
        "from common.gradient import numerical_gradient\n",
        "\n",
        "\n",
        "class simpleNet:\n",
        "    def __init__(self):\n",
        "        self.W = np.random.randn(2,3) # 정규분포로 초기화\n",
        "\n",
        "    def predict(self, x):\n",
        "        return np.dot(x, self.W)\n",
        "\n",
        "    def loss(self, x, t):\n",
        "        z = self.predict(x)\n",
        "        y = softmax(z)\n",
        "        loss = cross_entropy_error(y, t)\n",
        "\n",
        "        return loss\n",
        "\n",
        "x = np.array([0.6, 0.9])\n",
        "t = np.array([0, 0, 1])\n",
        "\n",
        "net = simpleNet()\n",
        "\n",
        "f = lambda w: net.loss(x, t)\n",
        "dW = numerical_gradient(f, net.W)\n",
        "\n",
        "print(dW)\n",
        "# 가져오기가 간혈적으로 됨....왜지"
      ],
      "metadata": {
        "id": "NYb4rMV74M4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "301427ae-cd80-48fa-bd0d-a22b6326d554"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.00600544  0.06204031 -0.06804575]\n",
            " [ 0.00900816  0.09306046 -0.10206862]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "여기서 말하는 기울기는 가중치 매개변수에 대한 손실 함수의 기울기 입니다."
      ],
      "metadata": {
        "id": "SQTgNtEVnYN7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 학습 알고리즘 구현하기"
      ],
      "metadata": {
        "id": "_oiPNwcCoKiN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "신경망 학습에는 다음 절차가 있다\n",
        "\n",
        "1단계-미니배치\n",
        "\n",
        "2단계-기울기 산출\n",
        "\n",
        "3단계- 매개변수 갱신(경사 하강법)\n",
        "\n",
        "4단계-반복"
      ],
      "metadata": {
        "id": "C2s8MzFB95Vy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "구현"
      ],
      "metadata": {
        "id": "wFS0gUdD_thW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, os\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks/deep-learning-from-scratch-master')  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
        "from common.functions import *\n",
        "from common.gradient import numerical_gradient\n",
        "\n",
        "\n",
        "class TwoLayerNet:\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
        "        # 가중치 초기화\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
        "        self.params['b1'] = np.zeros(hidden_size)\n",
        "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "        self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "    def predict(self, x):\n",
        "        W1, W2 = self.params['W1'], self.params['W2']\n",
        "        b1, b2 = self.params['b1'], self.params['b2']\n",
        "    \n",
        "        a1 = np.dot(x, W1) + b1\n",
        "        z1 = sigmoid(a1)\n",
        "        a2 = np.dot(z1, W2) + b2\n",
        "        y = softmax(a2)\n",
        "        \n",
        "        return y\n",
        "        \n",
        "    # x : 입력 데이터, t : 정답 레이블\n",
        "    def loss(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        \n",
        "        return cross_entropy_error(y, t)\n",
        "    \n",
        "    def accuracy(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        y = np.argmax(y, axis=1)\n",
        "        t = np.argmax(t, axis=1)\n",
        "        \n",
        "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "        return accuracy\n",
        "        \n",
        "    # x : 입력 데이터, t : 정답 레이블\n",
        "    def numerical_gradient(self, x, t):\n",
        "        loss_W = lambda W: self.loss(x, t)\n",
        "        \n",
        "        grads = {}\n",
        "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "        \n",
        "        return grads\n",
        "        \n",
        "net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
        "net.params['W1'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJAVWOeAoRbD",
        "outputId": "7e324611-b9a2-4697-d4bb-589648c9c92f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(784, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 시험 데이터로 평가하기"
      ],
      "metadata": {
        "id": "jftKNH3C7ZKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, os\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks/deep-learning-from-scratch-master')  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from two_layer_net import TwoLayerNet\n",
        "\n",
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "# 하이퍼파라미터\n",
        "iters_num = 10000  # 반복 횟수를 적절히 설정한다.\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100   # 미니배치 크기\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "# 1에폭당 반복 수\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    # 미니배치 획득\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "    \n",
        "    # 기울기 계산\n",
        "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
        "    grad = network.gradient(x_batch, t_batch)\n",
        "    \n",
        "    # 매개변수 갱신\n",
        "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "    \n",
        "    # 학습 경과 기록\n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "    \n",
        "    # 1에폭당 정확도 계산\n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "PZWzv0LWAyU2",
        "outputId": "6e00b624-607c-495e-c3bf-2243474040e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-57290845031a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_mnist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtwo_layer_net\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTwoLayerNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# 데이터 읽기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'two_layer_net'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}